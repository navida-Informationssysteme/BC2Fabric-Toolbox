{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "param_block",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------\n",
    "# Global Configuration / Parameters\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# The SQL Analytics Endpoint of your Fabric Lakehouse or Warehouse\n",
    "SQL_ENDPOINT = \"ABCXYZ123.datawarehouse.fabric.microsoft.com\"\n",
    "\n",
    "# The Database Name (Lakehouse or Warehouse name)\n",
    "DATABASE_NAME = \"bc2fabric_mirror\"\n",
    "\n",
    "# The Company Name filter for the Power BI Report\n",
    "COMPANY_NAME = \"CRONUS DE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f35e209",
   "metadata": {},
   "source": [
    "# Finance SQL Views Installer\n",
    "\n",
    "This notebook fetches the latest SQL definition from GitHub and executes it against the specified SQL Analytics Endpoint.\n",
    "\n",
    "**GitHub Source:** [finance-sql-views.sql](https://github.com/navida-Informationssysteme/BC2Fabric-Toolbox/blob/main/Accelerators/Finance-Reporting/finance-sql-views.sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7149b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pyodbc\n",
    "import struct\n",
    "import re\n",
    "\n",
    "# Try importing the modern notebookutils, fallback to mssparkutils if needed\n",
    "try:\n",
    "    from notebookutils import credentials\n",
    "except ImportError:\n",
    "    from mssparkutils import credentials\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 1. Fetch SQL Content\n",
    "# -------------------------------------------------------\n",
    "GITHUB_RAW_URL = \"https://raw.githubusercontent.com/navida-Informationssysteme/BC2Fabric-Toolbox/main/Accelerators/Finance-Reporting/finance-sql-views.sql\"\n",
    "\n",
    "print(f\"Fetching SQL from: {GITHUB_RAW_URL}\")\n",
    "try:\n",
    "    response = requests.get(GITHUB_RAW_URL)\n",
    "    response.raise_for_status()\n",
    "    sql_content = response.text\n",
    "    print(\"SQL content fetched successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to fetch SQL file: {e}\")\n",
    "    raise\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2. Connect to SQL Endpoint\n",
    "# -------------------------------------------------------\n",
    "# Get Entra ID (AAD) token\n",
    "token = credentials.getToken(\"pbi\")\n",
    "token_bytes = token.encode(\"UTF-16LE\")\n",
    "token_struct = struct.pack(f'<I{len(token_bytes)}s', len(token_bytes), token_bytes)\n",
    "SQL_COPT_SS_ACCESS_TOKEN = 1256 \n",
    "\n",
    "# Construct Connection String using Global Parameters\n",
    "conn_str = (\n",
    "    f\"Driver={{ODBC Driver 18 for SQL Server}};\"\n",
    "    f\"Server={SQL_ENDPOINT},1433;\"\n",
    "    f\"Database={DATABASE_NAME};\"\n",
    "    f\"Encrypt=yes;\"\n",
    "    f\"TrustServerCertificate=no;\"\n",
    "    f\"Connection Timeout=30;\"\n",
    ")\n",
    "\n",
    "print(f\"Connecting to {DATABASE_NAME}...\")\n",
    "\n",
    "try:\n",
    "    with pyodbc.connect(conn_str, attrs_before={SQL_COPT_SS_ACCESS_TOKEN: token_struct}, autocommit=True) as conn:\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        split_pattern = r'^\\s*GO\\s*;?\\s*$'\n",
    "        \n",
    "        statements = re.split(split_pattern, sql_content, flags=re.IGNORECASE | re.MULTILINE)\n",
    "        \n",
    "        # Filter out empty strings\n",
    "        statements = [stmt.strip() for stmt in statements if stmt.strip()]\n",
    "        \n",
    "        print(f\"Found {len(statements)} primary batches to execute.\")\n",
    "        \n",
    "        for i, stmt in enumerate(statements):\n",
    "            sub_statements = [stmt]\n",
    "            \n",
    "            if \"CREATE SCHEMA\" in stmt.upper() and \"CREATE OR ALTER VIEW\" in stmt.upper():\n",
    "                 # Lookahead split: Split right before \"CREATE OR ALTER VIEW\"\n",
    "                 sub_statements = re.split(r'(?=CREATE\\s+OR\\s+ALTER\\s+VIEW)', stmt, flags=re.IGNORECASE)\n",
    "\n",
    "            for sub_stmt in sub_statements:\n",
    "                clean_stmt = sub_stmt.strip()\n",
    "                if not clean_stmt: continue\n",
    "                \n",
    "                print(f\"Executing batch {i+1} part...\")\n",
    "                try:\n",
    "                    cursor.execute(clean_stmt)\n",
    "                except pyodbc.Error as e:\n",
    "                    print(f\"Error executing batch {i+1}:\")\n",
    "                    print(f\"Query start: {clean_stmt[:200]}...\") \n",
    "                    print(f\"Error details: {e}\")\n",
    "                    raise \n",
    "\n",
    "        print(\"All views created successfully.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(\"An error occurred during the database operation.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pbix_markdown_header",
   "metadata": {},
   "source": [
    "# Finance Report Installation\n",
    "\n",
    "This block performs the following actions:\n",
    "1. Downloads the Power BI Report (.pbix) template from GitHub.\n",
    "2. Imports it into the current Fabric Workspace.\n",
    "3. Updates the **Servername** and **Company** parameters using the global configuration defined at the top of this notebook.\n",
    "4. Triggers a dataset refresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pbix_logic_block",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# We still import credentials for the token, but we don't use mssparkutils for the Workspace ID anymore\n",
    "try:\n",
    "    from notebookutils import credentials\n",
    "except ImportError:\n",
    "    from mssparkutils import credentials\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Configuration\n",
    "# -------------------------------------------------------\n",
    "# 1. GitHub URL (Must be the \"Raw\" version of the link)\n",
    "GITHUB_PBIX_URL = \"https://github.com/navida-Informationssysteme/BC2Fabric-Toolbox/blob/main/Accelerators/Finance-Reporting/BC2Fabric_Finance_App.pbix?raw=1\"\n",
    "\n",
    "# 2. The Name you want for the report in Fabric\n",
    "REPORT_NAME = \"BC2Fabric_Finance_App\"\n",
    "\n",
    "# 3. The Workspace ID (Using Spark Config - Most Reliable Method)\n",
    "WORKSPACE_ID = spark.conf.get(\"trident.workspace.id\")\n",
    "\n",
    "# 4. Parameters to change after import { \"ParameterName\": \"NewValue\" }\n",
    "# We use the variables defined in the Global Parameters cell at the top of the notebook\n",
    "PARAMETERS_TO_UPDATE = {\n",
    "    \"Servername\": SQL_ENDPOINT,\n",
    "    \"Company\": COMPANY_NAME\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Helper Functions\n",
    "# -------------------------------------------------------\n",
    "def get_headers():\n",
    "    \"\"\"Get auth headers using the internal Fabric token\"\"\"\n",
    "    token = credentials.getToken(\"pbi\")\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "def poll_import_status(import_id, workspace_id):\n",
    "    \"\"\"Wait for the import to finish and return the new Dataset ID\"\"\"\n",
    "    url = f\"https://api.powerbi.com/v1.0/myorg/groups/{workspace_id}/imports/{import_id}\"\n",
    "    \n",
    "    print(\"Waiting for import to complete...\", end=\"\")\n",
    "    while True:\n",
    "        response = requests.get(url, headers=get_headers())\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        state = data.get(\"importState\")\n",
    "        \n",
    "        if state == \"Succeeded\":\n",
    "            print(\" Done!\")\n",
    "            return data[\"datasets\"][0][\"id\"]\n",
    "        elif state == \"Failed\":\n",
    "            # Print full error for debugging\n",
    "            print(f\"\\nImport Failed Details: {json.dumps(data, indent=2)}\")\n",
    "            raise Exception(f\"Import Failed: {data.get('error')}\")\n",
    "        \n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(2)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Main Execution\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# 1. Download PBIX\n",
    "print(f\"Downloading PBIX from: {GITHUB_PBIX_URL}\")\n",
    "file_response = requests.get(GITHUB_PBIX_URL)\n",
    "file_response.raise_for_status()\n",
    "print(f\"Download complete. File size: {len(file_response.content) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# 2. Upload (Import) to Workspace\n",
    "import_url = (\n",
    "    f\"https://api.powerbi.com/v1.0/myorg/groups/{WORKSPACE_ID}/imports\"\n",
    "    f\"?datasetDisplayName={REPORT_NAME}&nameConflict=CreateOrOverwrite\"\n",
    ")\n",
    "\n",
    "files = {\n",
    "    'file': (f'{REPORT_NAME}.pbix', file_response.content, 'application/octet-stream')\n",
    "}\n",
    "# Use a fresh token for the upload header\n",
    "upload_headers = {\"Authorization\": f\"Bearer {credentials.getToken('pbi')}\"}\n",
    "\n",
    "print(f\"Uploading to Workspace ID: {WORKSPACE_ID}...\")\n",
    "upload_resp = requests.post(import_url, headers=upload_headers, files=files)\n",
    "upload_resp.raise_for_status()\n",
    "import_id = upload_resp.json()[\"id\"]\n",
    "\n",
    "# 3. Wait for Import & Get Dataset ID\n",
    "dataset_id = poll_import_status(import_id, WORKSPACE_ID)\n",
    "print(f\"New Dataset ID: {dataset_id}\")\n",
    "\n",
    "# 4. Update Parameters\n",
    "if PARAMETERS_TO_UPDATE:\n",
    "    print(\"Updating Dataset Parameters...\")\n",
    "    \n",
    "    # The UpdateParameters API requires the dataset to be 'ready'.\n",
    "    # Sometimes it takes a split second after import succeeds to be writable.\n",
    "    time.sleep(5) \n",
    "    \n",
    "    update_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{WORKSPACE_ID}/datasets/{dataset_id}/Default.UpdateParameters\"\n",
    "    \n",
    "    update_payload = {\n",
    "        \"updateDetails\": [\n",
    "            {\"name\": key, \"newValue\": str(value)} \n",
    "            for key, value in PARAMETERS_TO_UPDATE.items()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        update_resp = requests.post(update_url, headers=get_headers(), json=update_payload)\n",
    "        update_resp.raise_for_status()\n",
    "        print(\"Parameters updated successfully.\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"Failed to update parameters: {e.response.text}\")\n",
    "        # We don't raise here to allow the refresh to attempt anyway\n",
    "    \n",
    "    # 5. Trigger Refresh\n",
    "    print(\"Triggering dataset refresh...\")\n",
    "    refresh_url = f\"https://api.powerbi.com/v1.0/myorg/groups/{WORKSPACE_ID}/datasets/{dataset_id}/refreshes\"\n",
    "    refresh_resp = requests.post(refresh_url, headers=get_headers())\n",
    "    refresh_resp.raise_for_status()\n",
    "    print(\"Refresh started successfully.\")\n",
    "\n",
    "print(\"Process Complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}